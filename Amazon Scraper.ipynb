{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Imports\n",
    "\n",
    "import requests # To get the page content\n",
    "from bs4 import BeautifulSoup as soup # To search and present page content\n",
    "from datetime import datetime as dt # Managing date formats\n",
    "import pandas as pd # Presenting data in CSV format\n",
    "import re # Regex for some string extraction (can replace soup easily)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pages based on search term or product page url\n",
    "\n",
    "BASE_URL = \"https://www.amazon.co.uk/s?k=\"\n",
    "\n",
    "PAGE_URL = \"https://www.amazon.com/dp\"\n",
    "\n",
    "# These headers make the scraper look like a normal user rather than a bot\n",
    "\n",
    "headers = {\n",
    "            'User-Agent':\n",
    "            'Mozilla/5.0 (X11; Linux x86_64) \\\n",
    "            AppleWebKit/537.36 (KHTML, like Gecko) \\\n",
    "            Chrome/47.0.2526.106 Safari/537.36'\n",
    "        }\n",
    "\n",
    "# Defining methods means we can use the code again without re-writing\n",
    "def get_search_results(path):\n",
    "    \n",
    "    # The path . replace is used to remove ' ' from the search query\n",
    "    \n",
    "    encoded_path = path.replace(\" \", \"+\")\n",
    "    \n",
    "    # requests is used to actually access the site and retrieve the HTML\n",
    "\n",
    "    page = requests.get(BASE_URL + encoded_path, headers=headers)\n",
    "    \n",
    "    # Soup (BeautifulSoup) allows for easy navigation and presentation\n",
    "    \n",
    "    search_results = soup(page.content, 'html.parser')\n",
    "\n",
    "    return search_results\n",
    "\n",
    "\n",
    "def get_product_page(page):\n",
    "    \n",
    "    html_page = requests.get(page, headers=headers)\n",
    "    \n",
    "    product_page = soup(html_page.content, 'html.parser')\n",
    "    \n",
    "    return product_page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_page_products(query):\n",
    "\n",
    "    search_results = get_search_results(query)\n",
    "    \n",
    "    # This is a BeautifulSoup search that looks for specific elements - returns a list\n",
    "    products = search_results.select(\"span[class=rush-component]\")\n",
    "    \n",
    "    # the [] defines a list of items that we can iterate through\n",
    "    search_products = []\n",
    "\n",
    "    x = 0\n",
    "\n",
    "    for product in products: # For every item in list (match for our select)\n",
    "\n",
    "        if str(product).__contains__('img'): # if the item in list contains text \"img\"\n",
    "            x += 1\n",
    "            try:\n",
    "                search_products.append( # append adds the below (a dictionary) to the list\n",
    "                    {\n",
    "                        'id': x,\n",
    "                        # Select is similar to a regex but using elements specifically\n",
    "                        'link': product.select(\"a[class=a-link-normal]\")[0]['href'],\n",
    "                        'name': product.select(\"img[class=s-image]\")[0]['alt'],\n",
    "                        'image': product.select(\"img[class=s-image]\")[0]['src'],\n",
    "                        'asin': re.findall(\n",
    "                            \".*dp/(.*?)/\", # The regex pattern we're looking to match\n",
    "                            product.select(\"a[class=a-link-normal]\")[0]['href']\n",
    "                        )[0] # [0] because re.findall returns a list but there's only 1\n",
    "                    }\n",
    "                )\n",
    "            except IndexError: \n",
    "                # The try and except handles the script erroring if there's no match\n",
    "                pass\n",
    "\n",
    "    return search_products      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(query):\n",
    "    pages = get_first_page_products(query)\n",
    "\n",
    "    product_reviews = []\n",
    "\n",
    "    for page in pages:\n",
    "\n",
    "        url = (\n",
    "\n",
    "            PAGE_URL \n",
    "            + \"/product-reviews/\"\n",
    "            + page['asin'] \n",
    "            + \"/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews\"\n",
    "        )\n",
    "\n",
    "        full_page = get_product_page(url)\n",
    "\n",
    "        pattern = re.compile(\"data-hook=\\\"review(.*?)review_comment_expander\", re.MULTILINE|re.DOTALL)\n",
    "        reviews = re.findall(pattern, str(full_page))\n",
    "        \n",
    "        try:\n",
    "\n",
    "            product_details = {\n",
    "                # Re is the python regex function\n",
    "                    'currency': re.findall(\".*a-color-price arp-price\\\"\\>(.).*?\\<\", str(full_page))[0],\n",
    "                    'price': re.findall(\".*a-color-price arp-price\\\"\\>.(.*?)\\<\", str(full_page))[0]\n",
    "            }\n",
    "        except IndexError: # If the regex doesn't find anything we skip it, rather than erroring.\n",
    "            pass\n",
    "\n",
    "        for review in reviews:\n",
    "\n",
    "            try:\n",
    "                product_reviews.append(\n",
    "                    {\n",
    "                        'asin': page['asin'],\n",
    "                        'name': page['name'],\n",
    "                        'link': page['link'],\n",
    "                        'image': page['image'],\n",
    "                        'currency': product_details['currency'],\n",
    "                        'price': product_details['price'],\n",
    "                        'date': dt.now(),\n",
    "                        'rating': re.findall(\"a-icon-alt\\\"\\>(.*?) out\", review, re.MULTILINE|re.DOTALL)[0],\n",
    "                        'text': re.findall(\"review-text-content\\\"><span class=\\\"\\\">(.*)</span>\", review, re.MULTILINE|re.DOTALL)[0],\n",
    "                        'upvotes': re.findall(\"<span class=\\\"review-votes\\\">(.*?)people\", review, re.MULTILINE|re.DOTALL)\n",
    "                    }\n",
    "                )\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "    return product_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search for Amazon product: Buzz lightyear\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Search for Amazon product: \")\n",
    "results = create_dataset(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
